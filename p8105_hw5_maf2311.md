p8105_hw5_maf2311
================
Mackenzie Frost (maf2311)

November 11, 2022

# Problem 1

A tidy dataframe containing data from all participants, including the
subject ID, arm, and observations over time:

Tidied result - file names include control arm and subject ID

``` r
final = 
  full_df %>% 
  mutate(
    names = str_replace(names, ".csv", ""),
    group = str_sub(names, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = names, week, outcome)
```

### Spaghetti plot showing observations on each subject over time

``` r
final %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

<img src="p8105_hw5_maf2311_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

In this plot, you can see the control group does not change much over
time, whereas the experimental group increases significantly by the end
of week 8.

# Problem 2

Homicides in 50 large US cities, Washington Post

``` r
homicides = read_csv(file = "./data-homicides-master/homicide-data.csv") %>%
  mutate(
    city_state = str_c(city, ", ", state)
  )
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
summary = homicides %>%
  group_by(city) %>%
  summarise(
    total_homicides = n(),
    unsolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest")
  ) %>%
  knitr::kable()

summary
```

| city           | total_homicides | unsolved |
|:---------------|----------------:|---------:|
| Albuquerque    |             378 |      146 |
| Atlanta        |             973 |      373 |
| Baltimore      |            2827 |     1825 |
| Baton Rouge    |             424 |      196 |
| Birmingham     |             800 |      347 |
| Boston         |             614 |      310 |
| Buffalo        |             521 |      319 |
| Charlotte      |             687 |      206 |
| Chicago        |            5535 |     4073 |
| Cincinnati     |             694 |      309 |
| Columbus       |            1084 |      575 |
| Dallas         |            1567 |      754 |
| Denver         |             312 |      169 |
| Detroit        |            2519 |     1482 |
| Durham         |             276 |      101 |
| Fort Worth     |             549 |      255 |
| Fresno         |             487 |      169 |
| Houston        |            2942 |     1493 |
| Indianapolis   |            1322 |      594 |
| Jacksonville   |            1168 |      597 |
| Kansas City    |            1190 |      486 |
| Las Vegas      |            1381 |      572 |
| Long Beach     |             378 |      156 |
| Los Angeles    |            2257 |     1106 |
| Louisville     |             576 |      261 |
| Memphis        |            1514 |      483 |
| Miami          |             744 |      450 |
| Milwaukee      |            1115 |      403 |
| Minneapolis    |             366 |      187 |
| Nashville      |             767 |      278 |
| New Orleans    |            1434 |      930 |
| New York       |             627 |      243 |
| Oakland        |             947 |      508 |
| Oklahoma City  |             672 |      326 |
| Omaha          |             409 |      169 |
| Philadelphia   |            3037 |     1360 |
| Phoenix        |             914 |      504 |
| Pittsburgh     |             631 |      337 |
| Richmond       |             429 |      113 |
| Sacramento     |             376 |      139 |
| San Antonio    |             833 |      357 |
| San Bernardino |             275 |      170 |
| San Diego      |             461 |      175 |
| San Francisco  |             663 |      336 |
| Savannah       |             246 |      115 |
| St. Louis      |            1677 |      905 |
| Stockton       |             444 |      266 |
| Tampa          |             208 |       95 |
| Tulsa          |             584 |      193 |
| Washington     |            1345 |      589 |

The raw data includes variables related to the record information,
victim information, location, and notes about the record. There are
52179 records and 12 variables. I then created a variable that combines
the city and the state information, to have 13 variables total.

For the city of Baltimore, MD, use the prop.test function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the broom::tidy to this object and pull
the estimated proportion and confidence intervals from the resulting
tidy dataframe.

Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a “tidy” pipeline, making use of purrr::map,
purrr::map2, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

Create a plot that shows the estimates and CIs for each city – check out
geom_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

# Problem 3

When designing an experiment or analysis, a common question is whether
it is likely that a true effect will be detected – put differently,
whether a false null hypothesis will be rejected. The probability that a
false null hypothesis is rejected is referred to as power, and it
depends on several factors, including: the sample size; the effect size;
and the error variance. In this problem, you will conduct a simulation
to explore power in a one-sample t-test.

First set the following design elements:

Fix n=30 Fix σ=5 Set μ=0. Generate 5000 datasets from the model

x∼Normal\[μ,σ\]

For each dataset, save μ̂ and the p-value arising from a test of H:μ=0
using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy
to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}, and complete the following:

Make a plot showing the proportion of times the null was rejected (the
power of the test) on the y axis and the true value of μ on the x axis.
Describe the association between effect size and power. Make a plot
showing the average estimate of μ̂ on the y axis and the true value of μ
on the x axis. Make a second plot (or overlay on the first) the average
estimate of μ̂ only in samples for which the null was rejected on the y
axis and the true value of μ on the x axis. Is the sample average of μ̂
across tests for which the null is rejected approximately equal to the
true value of μ? Why or why not?
